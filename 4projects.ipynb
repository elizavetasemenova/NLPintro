{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4: NLP mini-projects (Liza)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Text** is what we use to communicate. It documents how we talk and write. News as text is available and is relatively easy to scrap. Video and audio news can be transformed to text with the help of APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:coral\">Speech to text</span>\n",
    "\n",
    "Speech Recognition using **Google Speech API**\n",
    "\n",
    "We will perform a live demo, but not do this execrcise all together during the workshop, since it requires additional installations. Here is how they can be done:\n",
    "\n",
    "Installations (on Mac)\n",
    "\n",
    "```git clone http://people.csail.mit.edu/hubert/git/pyaudio.git```\n",
    "\n",
    "```cd pyaudio```\n",
    "\n",
    "```brew install portaudio```\n",
    "\n",
    "```pip install pyAudio```\n",
    "\n",
    "```pip install SpeechRecognition```\n",
    "\n",
    "Save the following code in a file speech2text.py and run in Termial with \n",
    "\n",
    "```python speech2text.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` #speech2text.py\n",
    "# Requires PyAudio and PySpeech.\n",
    "import speech_recognition as sr\n",
    " \n",
    "# Record Audio\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)\n",
    " \n",
    "# Speech recognition using Google Speech Recognition\n",
    "try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "    print(\"You said: \" + r.recognize_google(audio))\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The result should look as follows:\n",
    "\n",
    "![alt text](pics/banana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:coral\">Information retrieval (with spaCy)</span> </span> \n",
    "\n",
    "As discussed before, we can represent each text as a vector and evaluate the proximity of two texts as proximity of vectors. The [SpaCy](https://spacy.io/) library can help us do it neatly.\n",
    "\n",
    "Given a piece of news and a set of emails, we will extract the most relevant emails with respect to the news based on the **similarity** of texts. Similarity is determined by comparing word vectors or \"word embeddings\", multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec. Documentation can be found [here](https://spacy.io/usage/vectors-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Install and download:  \n",
    "# conda install spacy \n",
    "# python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def similarity(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    sim = round(doc1.similarity(doc2),2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1: ===================================================================\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do\n",
      "TEXT2: ===================================================================\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do\n"
     ]
    }
   ],
   "source": [
    "# a trivial example\n",
    "text_name_1 = 'data/easy_text1.txt'\n",
    "text_name_2 = 'data/easy_text1.txt'\n",
    "text1 = open(text_name_1).read()\n",
    "text2 = open(text_name_2).read()\n",
    "print('TEXT1: ===================================================================\\n' + text1)\n",
    "print('TEXT2: ===================================================================\\n' + text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of documents = 1.0\n"
     ]
    }
   ],
   "source": [
    "sim = similarity(text1, text2)\n",
    "print('Similarity of documents = ' +  str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1: ===================================================================\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do\n",
      "TEXT2: ===================================================================\n",
      "Alice was beginning to get very tired\n",
      "Similarity of documents = 0.72\n"
     ]
    }
   ],
   "source": [
    "# two diferent texts\n",
    "text_name_1 = 'data/easy_text1.txt'\n",
    "text_name_2 = 'data/easy_text2.txt'\n",
    "text1 = open(text_name_1).read()\n",
    "text2 = open(text_name_2).read()\n",
    "print('TEXT1: ===================================================================\\n' + text1)\n",
    "print('TEXT2: ===================================================================\\n' + text2)\n",
    "sim = similarity(text1, text2)\n",
    "print('Similarity of documents = ' +  str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imply that Hillary has a life- threatening condition called sinus thrombosis, helped create ISIS, and was responsible for the death of Americans in Benghazi\n"
     ]
    }
   ],
   "source": [
    "# read in the news text\n",
    "text_name = 'data/news1.txt'\n",
    "text_news = open(text_name).read()\n",
    "print(text_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read emails \n",
    "import json\n",
    "\n",
    "def load_json_data(path_to_file):\n",
    "    lol = pd.read_json(path_to_file,encoding='ascii')\n",
    "    data_DF = lol.T\n",
    "    data_DF['from'] = data_DF['from'].str.lower()\n",
    "    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n",
    "    #print(data_DF['body'])\n",
    "    return data_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Downloading the dataset\n",
    "```\n",
    "cd data\n",
    "curl https://www.dropbox.com/s/20suwbl2l287r54/fulldatastuff.json?dl=0 -L -o fulldatastuff.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emails_pd = load_json_data('data/fulldatastuff.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>from_name</th>\n",
       "      <th>subject</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many more states can we get to follow Conn...</td>\n",
       "      <td>2016-05-17T19:51:22-07:00</td>\n",
       "      <td>gardem@dnc.org</td>\n",
       "      <td>Maureen Garde</td>\n",
       "      <td>Re: CT To Automatically Register 400,000 Voters</td>\n",
       "      <td>[[\"Davis, Marilyn\", DavisM@dnc.org]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She maxed out to us earlier this year total un...</td>\n",
       "      <td>2016-05-04T06:58:23-07:00</td>\n",
       "      <td>shapiroa@dnc.org</td>\n",
       "      <td>\"Shapiro, Alexandra\"</td>\n",
       "      <td>What about asking Toni Bush to host?</td>\n",
       "      <td>[[\"Kaplan, Jordan\", KaplanJ@dnc.org]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>2016-05-04T16:49:31-04:00</td>\n",
       "      <td>postmaster@my.democrats.org</td>\n",
       "      <td>Contribution</td>\n",
       "      <td>Contribution: Finance - Tristate 2016 / Judith...</td>\n",
       "      <td>[[,, allenz@dnc.org], [,, parrishd@dnc.org], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Jordan KaplanNational Finance DirectorDemocrat...</td>\n",
       "      <td>2016-04-25T14:54:17-04:00</td>\n",
       "      <td>kaplanj@dnc.org</td>\n",
       "      <td>Jordan Kaplan</td>\n",
       "      <td>Re: Paris reception</td>\n",
       "      <td>[[\"Rauscher, Rachel\", RauscherR@dnc.org]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>nan</td>\n",
       "      <td>2016-05-04T05:47:44-06:00</td>\n",
       "      <td>illinoisplaybook@politico.com</td>\n",
       "      <td>Natasha Korecki</td>\n",
       "      <td>POLITICO Illinois Playbook, presented by Nucle...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  \\\n",
       "0     How many more states can we get to follow Conn...   \n",
       "1     She maxed out to us earlier this year total un...   \n",
       "10                                                  nan   \n",
       "100   Jordan KaplanNational Finance DirectorDemocrat...   \n",
       "1000                                                nan   \n",
       "\n",
       "                           date                           from  \\\n",
       "0     2016-05-17T19:51:22-07:00                 gardem@dnc.org   \n",
       "1     2016-05-04T06:58:23-07:00               shapiroa@dnc.org   \n",
       "10    2016-05-04T16:49:31-04:00    postmaster@my.democrats.org   \n",
       "100   2016-04-25T14:54:17-04:00                kaplanj@dnc.org   \n",
       "1000  2016-05-04T05:47:44-06:00  illinoisplaybook@politico.com   \n",
       "\n",
       "                 from_name                                            subject  \\\n",
       "0            Maureen Garde    Re: CT To Automatically Register 400,000 Voters   \n",
       "1     \"Shapiro, Alexandra\"               What about asking Toni Bush to host?   \n",
       "10            Contribution  Contribution: Finance - Tristate 2016 / Judith...   \n",
       "100          Jordan Kaplan                                Re: Paris reception   \n",
       "1000       Natasha Korecki  POLITICO Illinois Playbook, presented by Nucle...   \n",
       "\n",
       "                                                     to  \n",
       "0                  [[\"Davis, Marilyn\", DavisM@dnc.org]]  \n",
       "1                 [[\"Kaplan, Jordan\", KaplanJ@dnc.org]]  \n",
       "10    [[,, allenz@dnc.org], [,, parrishd@dnc.org], [...  \n",
       "100           [[\"Rauscher, Rachel\", RauscherR@dnc.org]]  \n",
       "1000                                                 []  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many more states can we get to follow Connecticut? Way to go!\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "emails = emails_pd['body'].tolist()\n",
    "print(emails[0])\n",
    "emails = emails[0:100]\n",
    "print(len(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN CASE YOU MISSED ITThe Only Time Donald Trump Undersells: Tax TimeABC NewsBy​ ​Brian RossMay 16, 2016http://abcnews.go.com/Politics/time-donald-trump-undersells-tax-time/story?id=39133709The Trump National Golf Club in Westchester County, New York, with its lovingly-manicured golf course, gently winding streams, stone bridges, 101-foot waterfall and an expansive clubhouse is, according to Donald Trump, reflective of “a true luxury lifestyle.”Creating such a “memorable club” is not cheap -- Trump wrote on a candidate disclosure form that the sprawling 147-acre private club bearing his name is worth “more than $50 million.”But when it came time to value the property for tax purposes, his lawyers have argued that Trump National is really only worth $1.35 million. The proposed valuation has bewildered officials in the small town of Ossining, who said the new figure would cut Trump’s tax burden by 90 percent and dump that burden on everyone else.“Trump says he represents the little guy, b\n"
     ]
    }
   ],
   "source": [
    "# for a given news piece iterate through all emails and find ntop relevant ones\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "similarities = defaultdict(int)\n",
    "\n",
    "def similar_emails(text_news, emails, ntop=1):\n",
    "    for i in range(len(emails)):\n",
    "        email = emails[i]\n",
    "        similarities[i] = similarity(text_news, email)\n",
    "    similarities_index = nlargest(ntop, similarities, key=similarities.get)\n",
    "    return [emails[similarities_index[i]] for i in range(ntop)]\n",
    "\n",
    "relevant_emails = similar_emails(text_news, emails, ntop=1)\n",
    "print(relevant_emails[0][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Documentation:\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:coral\">Text summarization (with NLTK)</span>\n",
    "\n",
    "Another great library to work with texts is [NLTK](http://www.nltk.org/), which stands for Natural Language Toolkit. We have now extracted a large set of news on a given topic and would like to extract the most informative parts: out of each text we would like to exatract the most informative sentence. This type of summarization of texts is called **extractive**.\n",
    "\n",
    "Take a look at this video: [Hillary Clinton's concession speech](https://www.vox.com/2016/11/9/13570328/hillary-clinton-concession-speech-full-transcript-2016-presidential-election). The webpage already provides the full transcript.\n",
    "\n",
    "\n",
    "\n",
    "We will approach this task as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 6390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thank you. thank you all very much. thank you so much.\\n\\nvery rowdy group. thank you, my friends. thank you. thank you. thank you so very much for being here. i love you all, too.\\n\\nlast night i congratulated donald trump and offered to work with him on behalf of our country.\\n\\ni hope that he will be a'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import text\n",
    "\n",
    "text=open('data/trainhillary.txt').read().lower().replace('\\xa0',' ')\n",
    "print('corpus length:', len(text))\n",
    "text[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you.',\n",
       " 'thank you all very much.',\n",
       " 'thank you so much.',\n",
       " 'very rowdy group.',\n",
       " 'thank you, my friends.',\n",
       " 'thank you.',\n",
       " 'thank you.',\n",
       " 'thank you so very much for being here.',\n",
       " 'i love you all, too.',\n",
       " 'last night i congratulated donald trump and offered to work with him on behalf of our country.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "word_sent = [nltk.word_tokenize(s.lower()) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thank', 'you', '.'],\n",
       " ['thank', 'you', 'all', 'very', 'much', '.'],\n",
       " ['thank', 'you', 'so', 'much', '.'],\n",
       " ['very', 'rowdy', 'group', '.'],\n",
       " ['thank', 'you', ',', 'my', 'friends', '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sent[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute frequencies \n",
    "freq = defaultdict(int)\n",
    "for sentence in word_sent:\n",
    "    for word in sentence:\n",
    "        if word not in stopWords:\n",
    "            freq[word] +=1\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = float(max(freq.values()))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for word in freq.keys():\n",
    "    freq[word] = freq[word]/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "min_cut=0.2\n",
    "max_cut=0.8\n",
    "freq_new = defaultdict(int)\n",
    "for word in freq.keys():\n",
    "    if not freq[word] > max_cut or freq[word] < min_cut:\n",
    "        freq_new[word] = freq[word]\n",
    "freq = freq_new\n",
    "del freq_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ranking = defaultdict(int)\n",
    "for i, sentence in enumerate(word_sent):\n",
    "        for word in sentence:\n",
    "            if word in freq:\n",
    "                ranking[i] +=freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'we’ve spent a year and a half bringing together millions of people from every corner of our country to say with one voice that we believe that the american dream is big enough for everyone—for people of all races, and religions, for men and women, for immigrants, for lgbt people, and people with disabilities.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "sentences_index = nlargest(1, ranking, key=ranking.get)\n",
    "print(sentences_index)\n",
    "sentences[sentences_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## All of it in one function\n",
    "from heapq import nlargest\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_text(text, stopWords, min_cut, max_cut, ntop=1):\n",
    "   \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    word_sent = [nltk.word_tokenize(s.lower()) for s in sentences]\n",
    "    \n",
    "    # compute frequencies \n",
    "    freq = defaultdict(int)\n",
    "    for sentence in word_sent:\n",
    "        for word in sentence:\n",
    "            if word not in stopWords:\n",
    "                freq[word] +=1\n",
    "\n",
    "    # normilize frequencies \n",
    "    m = float(max(freq.values()))\n",
    "    for word in freq.keys():\n",
    "        freq[word] = freq[word]/m\n",
    " \n",
    "    # cut off too frequent or too rare words\n",
    "    freq_new = defaultdict(int)\n",
    "    for word in freq.keys():\n",
    "        if not freq[word] >= max_cut or freq[word] <= min_cut:\n",
    "            freq_new[word] = freq[word]\n",
    "    freq = freq_new\n",
    "    del freq_new\n",
    "    \n",
    "    # rank sentences\n",
    "    ranking = defaultdict(int)\n",
    "    for i, sentence in enumerate(word_sent):\n",
    "        for word in sentence:\n",
    "            if word in freq:\n",
    "                ranking[i] +=freq[word]\n",
    "                \n",
    "    sentences_index = nlargest(ntop, ranking, key=ranking.get)\n",
    "    summary = [sentences[sentences_index[ind]] for ind in range(len(sentences_index))]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIZING SENTENCE : \n",
      "we’ve spent a year and a half bringing together millions of people from every corner of our country to say with one voice that we believe that the american dream is big enough for everyone—for people of all races, and religions, for men and women, for immigrants, for lgbt people, and people with disabilities.\n"
     ]
    }
   ],
   "source": [
    "min_cut = 0.2\n",
    "max_cut = 0.8\n",
    "ntop = 1\n",
    "summary = summarize_text(text, stopWords, min_cut, max_cut, ntop)\n",
    "print('SUMMARIZING SENTENCE : \\n' + str(summary[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:amld]",
   "language": "python",
   "name": "conda-env-amld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
